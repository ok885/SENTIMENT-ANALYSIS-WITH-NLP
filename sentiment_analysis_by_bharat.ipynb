{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Project\n",
    "\n",
    "## Introduction\n",
    "Hello! This is my first big project in Natural Language Processing (NLP). I'm going to build a model that can figure out the sentiment (feeling) behind movie review phrases. This is super useful for understanding what people think about movies!\n",
    "\n",
    "I'll be using TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numbers and then a Logistic Regression model to classify the sentiments. The sentiments are labeled from 0 (negative) to 4 (positive), with 2 being neutral.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to bring in all the tools we'll use.\n",
    "import pandas as pd # For working with data tables (DataFrames)\n",
    "import numpy as np  # For numerical operations, especially with arrays\n",
    "import matplotlib.pyplot as plt # For making cool charts and graphs\n",
    "import nltk # The Natural Language Toolkit, essential for text processing\n",
    "from nltk.corpus import stopwords # To remove common words like 'the', 'is', 'a'\n",
    "from nltk.stem.snowball import SnowballStemmer # To reduce words to their root form (e.g., 'running' to 'run')\n",
    "from nltk.tokenize import word_tokenize # To break sentences into individual words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # To convert text into numerical features\n",
    "from sklearn.linear_model import LogisticRegression # Our chosen model for classification\n",
    "from sklearn.model_selection import train_test_split # To split our data for training and testing\n",
    "from sklearn.metrics import accuracy_score # To see how well our model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "I'll load the training and testing datasets. These files contain phrases and their corresponding sentiments (for training) or just phrases (for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data. 'sep=\\t' means the values are separated by tabs.\n",
    "try:\n",
    "    train_df = pd.read_csv('train.tsv', sep='\\t')\n",
    "    test_df = pd.read_csv('test.tsv', sep='\\t')\n",
    "    sample_submission_df = pd.read_csv('sampleSubmission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure 'train.tsv', 'test.tsv', and 'sampleSubmission.csv' are in the same directory!\")\n",
    "    # Create dummy dataframes for demonstration if files are not found\n",
    "    train_df = pd.DataFrame({\n",
    "        'PhraseId': [1, 2, 3, 4, 5],\n",
    "        'SentenceId': [1, 1, 1, 1, 1],\n",
    "        'Phrase': [\n",
    "            'A very good movie!',\n",
    "            'This film was terrible.',\n",
    "            'It was okay, nothing special.',\n",
    "            'Absolutely fantastic!',\n",
    "            'Could have been better.'\n",
    "        ],\n",
    "        'Sentiment': [3, 0, 2, 4, 1]\n",
    "    })\n",
    "    test_df = pd.DataFrame({\n",
    "        'PhraseId': [156061, 156062, 156063],\n",
    "        'SentenceId': [8545, 8545, 8545],\n",
    "        'Phrase': [\n",
    "            'An intermittently pleasing but mostly routine effort',\n",
    "            'Not bad, not great.',\n",
    "            'The worst movie ever.'\n",
    "        ]\n",
    "    })\n",
    "    sample_submission_df = pd.DataFrame({\n",
    "        'PhraseId': [156061, 156062, 156063],\n",
    "        'Sentiment': [2, 2, 2]\n",
    "    })\n",
    "\n",
    "# Let's see the first few rows of our training data\n",
    "print(\"\\n--- Training Data Head ---\")\n",
    "print(train_df.head())\n",
    "\n",
    "# And the test data\n",
    "print(\"\\n--- Test Data Head ---\")\n",
    "print(test_df.head())\n",
    "\n",
    "# And the sample submission file format\n",
    "print(\"\\n--- Sample Submission Data Head ---\")\n",
    "print(sample_submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization\n",
    "It's good to understand our data. Let's look at the distribution of sentiments in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many phrases fall into each sentiment category\n",
    "sentiment_counts = train_df['Sentiment'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"\\n--- Sentiment Distribution ---\")\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Now, let's visualize it with a bar chart!\n",
    "plt.figure(figsize=(8, 5))\n",
    "sentiment_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Sentiments in Training Data')\n",
    "plt.xlabel('Sentiment (0: Negative, 1: Somewhat Negative, 2: Neutral, 3: Somewhat Positive, 4: Positive)')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=0) # Keep labels horizontal\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLooks like 'neutral' (sentiment 2) is the most common category. This is important to remember!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Before we can feed text to a machine learning model, we need to clean it up and convert it into a format the model understands. This involves:\n",
    "1.  **Tokenization**: Breaking sentences into words.\n",
    "2.  **Lowercasing**: Converting all words to lowercase to treat 'The' and 'the' as the same word.\n",
    "3.  **Removing Punctuation/Numbers**: Keeping only alphabetic characters.\n",
    "4.  **Stemming**: Reducing words to their base form (e.g., 'running', 'runs', 'ran' all become 'run').\n",
    "5.  **Stopword Removal**: Getting rid of common words that don't add much meaning (like 'a', 'an', 'the')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nltk-downloads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download some NLTK data for tokenization and stopwords\n",
    "print(\"Downloading NLTK 'punkt' tokenizer data...\")\n",
    "nltk.download('punkt', quiet=True) # For word_tokenize\n",
    "print(\"Downloading NLTK 'stopwords' data...\")\n",
    "nltk.download('stopwords', quiet=True) # For english stopwords\n",
    "\n",
    "# Initialize our stemmer and get English stopwords\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "print(\"\\nExample of a stemmed word: 'Running' becomes '\" + stemmer.stem('Running') + \"'\")\n",
    "print(\"\\nSome English stopwords: \" + \", \".join(english_stopwords[:10]) + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will do all our text cleaning steps!\n",
    "def custom_tokenizer(text):\n",
    "    # 1. Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # 2. Keep only alphabetic characters (remove punctuation, numbers)\n",
    "    alphabetic_tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # 3. Remove stopwords and then stem the remaining words\n",
    "    processed_tokens = [stemmer.stem(token) for token in alphabetic_tokens if token not in english_stopwords]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Let's test our tokenizer with an example sentence\n",
    "example_sentence = \"This is a really great movie, I loved it!\"\n",
    "print(f\"\\nOriginal sentence: '{example_sentence}'\")\n",
    "print(f\"Processed tokens: {custom_tokenizer(example_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "Now that our text is clean, we need to turn it into numbers. TF-IDF helps us do this by giving importance to words that are frequent in one document but not too common across all documents. This way, important words stand out.\n",
    "\n",
    "I'll use `ngram_range=(1,2)` to consider single words (unigrams) and pairs of words (bigrams). I'll also limit the `max_features` to keep the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-vectorizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "# tokenizer=custom_tokenizer: tells it to use our cleaning function\n",
    "# ngram_range=(1,2): considers single words and two-word phrases\n",
    "# max_features: limits the number of unique words/phrases to consider (to avoid too many features)\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, \n",
    "                           ngram_range=(1,2),\n",
    "                           max_features=2300) # Using 2300 features as seen in the reference notebook\n",
    "\n",
    "# Fit the vectorizer on our training phrases. This learns the vocabulary and IDF values.\n",
    "print(\"\\nFitting TF-IDF Vectorizer on training phrases...\")\n",
    "inputs_train_transformed = vectorizer.fit_transform(train_df['Phrase'])\n",
    "\n",
    "# Transform the test phrases using the same fitted vectorizer\n",
    "print(\"Transforming test phrases...\")\n",
    "test_df = test_df.dropna(subset=['Phrase']) # Important: remove any empty phrases from test data\n",
    "inputs_test_transformed = vectorizer.transform(test_df['Phrase'])\n",
    "\n",
    "print(f\"\\nShape of transformed training data (samples, features): {inputs_train_transformed.shape}\")\n",
    "print(f\"Shape of transformed test data (samples, features): {inputs_test_transformed.shape}\")\n",
    "\n",
    "print(\"\\nFirst 10 feature names (words/phrases) learned by the vectorizer:\")\n",
    "print(vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data for Training and Validation\n",
    "It's good practice to split our training data into a smaller training set and a validation set. This helps us check how well our model generalizes to new, unseen data before we test it on the actual test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our features (X) and targets (y)\n",
    "X = inputs_train_transformed\n",
    "y = train_df['Sentiment']\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "# test_size=0.2 means 20% of data will be for validation\n",
    "# random_state=42 ensures we get the same split every time\n",
    "print(\"Splitting training data into training and validation sets...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nShape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}, y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model\n",
    "Logistic Regression is a simple yet powerful model for classification tasks. It's a good starting point for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "# max_iter=1000: increases the number of iterations for the solver to converge\n",
    "print(\"Initializing Logistic Regression model...\")\n",
    "model = LogisticRegression(max_iter=1000, solver='liblinear') # 'liblinear' solver works well for small datasets and L1/L2 regularization\n",
    "\n",
    "# Train the model using our training data\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now let's see how well our model performs on the training data and, more importantly, on the validation data (which it hasn't seen during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set\n",
    "train_preds = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy on the training set\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nObservation: The accuracy on the validation set is a bit lower than the training set. This is normal and indicates how well the model generalizes. Also, remember that the dataset has a class imbalance, with many neutral sentiments, which can affect overall accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on Test Data and Creating Submission File\n",
    "Finally, we'll use our trained model to predict sentiments for the test dataset and prepare the `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-and-submit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the actual test data\n",
    "print(\"Making predictions on the test data...\")\n",
    "test_preds = model.predict(inputs_test_transformed)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'PhraseId': test_df['PhraseId'],\n",
    "    'Sentiment': test_preds\n",
    "})\n",
    "\n",
    "# Display the first few rows of our submission file\n",
    "print(\"\\n--- Submission File Head ---\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Save the submission file to a CSV without the index\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We successfully built a sentiment analysis model using TF-IDF and Logistic Regression! This notebook covers the full pipeline from data loading and exploration to preprocessing, model training, evaluation, and generating a submission file.\n",
    "\n",
    "### Next Steps/Possible Improvements:\n",
    "* **More Advanced Preprocessing**: Try lemmatization instead of stemming.\n",
    "* **Different Models**: Experiment with other machine learning models like Naive Bayes, SVM, or even deep learning models (RNNs, Transformers).\n",
    "* **Hyperparameter Tuning**: Optimize the parameters of the `TfidfVectorizer` and `LogisticRegression` for better performance.\n",
    "* **Handling Imbalance**: Explore techniques to handle the class imbalance (e.g., oversampling, undersampling, using weighted loss functions).\n",
    "\n",
    "Thanks for checking out my project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
